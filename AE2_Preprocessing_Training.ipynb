{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1f67d3",
   "metadata": {},
   "source": [
    "## 2) Data Collection and Pre-Processing\n",
    "- I am using the .txt files from the following books: \n",
    "- I am combining them in a list and pre-processing them\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4d800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import certifi\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import contractions\n",
    "import unicodedata\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from flask import Flask, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07708973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/julianniestroj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julianniestroj/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45aecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = ['adventures.txt', 'crime_and_punishment.txt', 'don_quijote.txt', 'frankenstein.txt', 'great_expectations.txt', 'history_of_tom_jones.txt', 'jane_eyre.txt', 'leviathan.txt', 'makers.txt', 'middlemarch.txt', 'moby_dick.txt', 'my_life.txt', 'pride_and_prejudice.txt', 'shakespeare_complete_works.txt', 'the_count_of_monte_cristo.txt', 'twenty_years_after.txt', 'ulysses.txt', 'war_and_peace.txt']\n",
    "\n",
    "for txt_file in books:\n",
    "    with open(txt_file, 'r', encoding = 'utf-8') as file:\n",
    "        content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a15a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbcaeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "\n",
    "for txt_file in books:\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "        # Converting UTF characters\n",
    "        content = convert_utf(content)\n",
    "        # Lowercasing\n",
    "        content = content.lower()\n",
    "        # Expanding contractions\n",
    "        content = ' '.join([contractions.fix(word) for word in content.split()])\n",
    "\n",
    "        # Sentence tokenization\n",
    "        sentences = sent_tokenize(content)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Word tokenization\n",
    "            words = word_tokenize(sentence)\n",
    "\n",
    "            # Remove punctuation\n",
    "            words = [word for word in words if word.isalpha()]\n",
    "\n",
    "            all_sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a8953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'roderick', 'random', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever']\n",
      "Sentence 2: ['you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n",
      "Sentence 3: ['if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook']\n",
      "Sentence 4: ['title', 'the', 'adventures', 'of', 'roderick', 'random', 'author', 'smollett', 'release', 'date', 'may', 'ebook', 'most', 'recently', 'updated', 'august', 'language', 'english', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'adventures', 'of', 'roderick', 'random', 'the', 'adventures', 'of', 'roderick', 'random', 'by', 'tobias', 'smollett', 'contents', 'the', 'author', 'preface', 'apologue', 'chapter', 'chapter', 'ii']\n",
      "Sentence 5: ['chapter', 'iii']\n"
     ]
    }
   ],
   "source": [
    "# Checking the first few processed sentences:\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i+1}: {all_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "253efb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-lowercase words found.\n"
     ]
    }
   ],
   "source": [
    "# Checking if any character is not a lowercase letter:\n",
    "\n",
    "for sentence in all_sentences:\n",
    "    for word in sentence:\n",
    "        if not word.islower():\n",
    "            print(f\"Non-lowercase word found: {word}\")\n",
    "\n",
    "# Print a message if no issues are found\n",
    "print(\"No non-lowercase words found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3dc33",
   "metadata": {},
   "source": [
    "## 3) Training[20marks]:\n",
    "● Use a Word2Vec embeddings technique. (10 marks)\n",
    "● Utilise Gensim library to assist with the training.\n",
    "● Save the trained model for future use. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcee00",
   "metadata": {},
   "source": [
    "#### Step 1: Import Gensim and Prepare the Model\n",
    "- First, import the necessary Gensim modules and create an instance of the Word2Vec model.vector_size: The number of dimensions of the embeddings.\n",
    "- window: The maximum distance between the current and predicted word within a sentence.\n",
    "- min_count: Ignores all words with total frequency lower than this.\n",
    "- sg: Training algorithm: 1 for skip-gram; otherwise CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2d3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Word2Vec model\n",
    "model = gensim.models.Word2Vec(vector_size=200, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d304b5b",
   "metadata": {},
   "source": [
    "#### Step 2: Build Vocabulary\n",
    "- Load your preprocessed data (the all_sentences list) and build the model's vocabulary.\n",
    "- all_sentences is the list of tokenized sentences you prepared.\n",
    "- update=False means the vocabulary is initialized for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce804c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(all_sentences, update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4e229",
   "metadata": {},
   "source": [
    "#### Step 3: Train the Model with the sentences.\n",
    "- total_examples: Count of sentences.\n",
    "- epochs: Number of iterations over the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3118c4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20167989, 26583320)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(all_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f725f",
   "metadata": {},
   "source": [
    "#### Step 4: Save the Model\n",
    "-Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1836a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_su')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b45f8b56",
   "metadata": {},
   "source": [
    "#### Step 5: Test the Model\n",
    "- Check how the model performs with an example.\n",
    "- This prints the words most similar to 'man'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72991e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.7586867809295654), ('gentleman', 0.7285850048065186), ('soldier', 0.6747897863388062), ('fellow', 0.6522435545921326), ('creature', 0.6492232084274292)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('man', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce3172",
   "metadata": {},
   "source": [
    "Below I have the same preprocessing for the model, but it additionally removes stopwords. I want to see if my model is more accurate with or without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc57227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "\n",
    "all_sentences_no_stopwords = []\n",
    "counter = 0\n",
    "for txt_file in books:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "        # Converting UTF characters and preprocess\n",
    "        content = convert_utf(content)\n",
    "        content = content.lower()\n",
    "\n",
    "        # Sentence tokenization\n",
    "        sentences = sent_tokenize(content)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Word tokenization\n",
    "            words = word_tokenize(sentence)\n",
    "\n",
    "            # Removing punctuation and stopwords\n",
    "            words = [word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
    "\n",
    "            all_sentences_no_stopwords.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca5a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(all_sentences_no_stopwords, update=False)\n",
    "model.train(all_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('model_su_no_stop')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WURthesis",
   "language": "python",
   "name": "wurthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
